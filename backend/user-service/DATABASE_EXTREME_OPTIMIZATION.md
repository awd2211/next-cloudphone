# æ•°æ®åº“æè‡´æ€§èƒ½ä¼˜åŒ–æ–¹æ¡ˆ

**ç›®æ ‡**: å°†æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–åˆ°æè‡´,æ”¯æŒ 100 ä¸‡ç”¨æˆ· + 1000 ä¸‡äº‹ä»¶
**é¢„æœŸæå‡**: æ•´ä½“æ€§èƒ½æå‡ 300%+

---

## ä¼˜åŒ–ç­–ç•¥æ¦‚è§ˆ

| ä¼˜åŒ–é¡¹ | å½“å‰æ€§èƒ½ | ç›®æ ‡æ€§èƒ½ | æå‡ |
|--------|---------|---------|------|
| ç”¨æˆ·ç™»å½• | 50ms | 20ms | 60% â¬†ï¸ |
| é…é¢æ£€æŸ¥ | 30ms | 3ms | 90% â¬†ï¸ |
| ç»Ÿè®¡æŸ¥è¯¢ | 150ms | 30ms | 80% â¬†ï¸ |
| äº‹ä»¶é‡æ”¾ | 100ms | 30ms | 70% â¬†ï¸ |
| å¹¶å‘è¿æ¥ | 50 | 200 | 300% â¬†ï¸ |
| æ•°æ®åº“æŸ¥è¯¢ | 100 qps | 1000 qps | 900% â¬†ï¸ |

---

## ç¬¬ä¸€é˜¶æ®µ: å…³é”®ç´¢å¼•å’Œçº¦æŸä¼˜åŒ–

### 1.1 æ·»åŠ äº‹ä»¶ç‰ˆæœ¬å”¯ä¸€çº¦æŸ (HIGH PRIORITY)

**é—®é¢˜**: å½“å‰åªåœ¨åº”ç”¨å±‚æ£€æŸ¥ç‰ˆæœ¬å†²çª,æ•°æ®åº“å±‚æ²¡æœ‰ä¿è¯

**è§£å†³æ–¹æ¡ˆ**:
```sql
-- æ·»åŠ å”¯ä¸€çº¦æŸé˜²æ­¢å¹¶å‘å†²çª
ALTER TABLE user_events
ADD CONSTRAINT uk_user_event_aggregate_version
UNIQUE (aggregate_id, version);

-- æ·»åŠ æ£€æŸ¥çº¦æŸç¡®ä¿ç‰ˆæœ¬ä¸ºæ­£æ•°
ALTER TABLE user_events
ADD CONSTRAINT ck_user_event_version_positive
CHECK (version > 0);
```

**æ€§èƒ½å½±å“**:
- é˜²æ­¢åŒé‡å†™å…¥ âœ…
- æ•°æ®åº“å±‚é¢ä¿è¯ä¸€è‡´æ€§ âœ…
- ç•¥å¾®å¢åŠ å†™å…¥å¼€é”€ (< 2%) âš ï¸

### 1.2 æ·»åŠ éƒ¨åˆ†ç´¢å¼•ä¼˜åŒ–çƒ­ç‚¹æŸ¥è¯¢

```sql
-- åªç´¢å¼•æ´»è·ƒç”¨æˆ· (å‡å°‘ç´¢å¼•å¤§å° 80%)
CREATE INDEX idx_users_active_created
ON users(created_at)
WHERE status = 'active';

-- åªç´¢å¼•æœªè¿‡æœŸé…é¢
CREATE INDEX idx_quotas_active_valid
ON quotas(user_id, valid_until)
WHERE status = 'active' AND valid_until > NOW();

-- åªç´¢å¼•æœ€è¿‘ 90 å¤©çš„å®¡è®¡æ—¥å¿—
CREATE INDEX idx_audit_logs_recent
ON audit_logs(user_id, created_at DESC)
WHERE created_at > NOW() - INTERVAL '90 days';
```

**æ€§èƒ½æå‡**:
- ç´¢å¼•å¤§å°å‡å°‘ 70-80%
- æŸ¥è¯¢é€Ÿåº¦æå‡ 40-60%
- å†™å…¥é€Ÿåº¦æå‡ 20-30%

### 1.3 æ·»åŠ è¦†ç›–ç´¢å¼•

```sql
-- ç”¨æˆ·åˆ—è¡¨æŸ¥è¯¢è¦†ç›–ç´¢å¼• (é¿å…å›è¡¨æŸ¥è¯¢)
CREATE INDEX idx_users_list_covering
ON users(tenant_id, created_at DESC)
INCLUDE (id, username, email, full_name, status);

-- äº‹ä»¶ç»Ÿè®¡è¦†ç›–ç´¢å¼•
CREATE INDEX idx_user_events_stats_covering
ON user_events(event_type, tenant_id, created_at)
INCLUDE (aggregate_id);
```

**æ€§èƒ½æå‡**:
- å‡å°‘ç£ç›˜ I/O 50-70%
- æŸ¥è¯¢é€Ÿåº¦æå‡ 30-50%

---

## ç¬¬äºŒé˜¶æ®µ: è¡¨åˆ†åŒºç­–ç•¥

### 2.1 user_events è¡¨æŒ‰æœˆåˆ†åŒº

```sql
-- 1. åˆ›å»ºåˆ†åŒºè¡¨ç»“æ„
CREATE TABLE user_events_partitioned (
  LIKE user_events INCLUDING ALL
) PARTITION BY RANGE (created_at);

-- 2. åˆ›å»ºæœªæ¥ 12 ä¸ªæœˆçš„åˆ†åŒº
CREATE TABLE user_events_2025_10 PARTITION OF user_events_partitioned
FOR VALUES FROM ('2025-10-01') TO ('2025-11-01');

CREATE TABLE user_events_2025_11 PARTITION OF user_events_partitioned
FOR VALUES FROM ('2025-11-01') TO ('2025-12-01');

-- ... (åˆ›å»ºåˆ° 2026-10)

-- 3. è¿ç§»æ•°æ® (åœ¨ç»´æŠ¤çª—å£æ‰§è¡Œ)
BEGIN;
INSERT INTO user_events_partitioned SELECT * FROM user_events;
ALTER TABLE user_events RENAME TO user_events_old;
ALTER TABLE user_events_partitioned RENAME TO user_events;
COMMIT;

-- 4. è‡ªåŠ¨åˆ†åŒºåˆ›å»ºå‡½æ•°
CREATE OR REPLACE FUNCTION create_user_event_partitions()
RETURNS void AS $$
DECLARE
  partition_date DATE;
  partition_name TEXT;
  start_date TEXT;
  end_date TEXT;
BEGIN
  -- åˆ›å»ºæœªæ¥ 3 ä¸ªæœˆçš„åˆ†åŒº
  FOR i IN 1..3 LOOP
    partition_date := DATE_TRUNC('month', CURRENT_DATE + (i || ' month')::INTERVAL);
    partition_name := 'user_events_' || TO_CHAR(partition_date, 'YYYY_MM');
    start_date := TO_CHAR(partition_date, 'YYYY-MM-DD');
    end_date := TO_CHAR(partition_date + INTERVAL '1 month', 'YYYY-MM-DD');

    -- æ£€æŸ¥åˆ†åŒºæ˜¯å¦å·²å­˜åœ¨
    IF NOT EXISTS (
      SELECT 1 FROM pg_tables
      WHERE tablename = partition_name
    ) THEN
      EXECUTE format(
        'CREATE TABLE %I PARTITION OF user_events FOR VALUES FROM (%L) TO (%L)',
        partition_name, start_date, end_date
      );
      RAISE NOTICE 'Created partition: %', partition_name;
    END IF;
  END LOOP;
END;
$$ LANGUAGE plpgsql;

-- 5. å®šæ—¶ä»»åŠ¡ (æ¯æœˆ 1 å·æ‰§è¡Œ)
-- åœ¨åº”ç”¨å±‚é€šè¿‡ cron job æˆ– pg_cron è°ƒç”¨
```

**æ€§èƒ½æå‡**:
- æŸ¥è¯¢é€Ÿåº¦: å¿« 60-80% (åªæ‰«æç›¸å…³åˆ†åŒº)
- ç´¢å¼•ç»´æŠ¤: å¿« 70% (åˆ†åŒºç‹¬ç«‹ç´¢å¼•)
- å½’æ¡£æ“ä½œ: ç§’çº§åˆ é™¤ (DROP PARTITION)

### 2.2 audit_logs è¡¨æŒ‰æœˆåˆ†åŒº

```sql
-- å®¡è®¡æ—¥å¿—åŒæ ·æŒ‰æœˆåˆ†åŒº
CREATE TABLE audit_logs_partitioned (
  LIKE audit_logs INCLUDING ALL
) PARTITION BY RANGE (created_at);

-- åˆ›å»ºåˆ†åŒº (åŒä¸Š)
-- ä¿ç•™æœ€è¿‘ 12 ä¸ªæœˆ,è‡ªåŠ¨å½’æ¡£æ—§åˆ†åŒº
```

---

## ç¬¬ä¸‰é˜¶æ®µ: è¿æ¥æ± æè‡´ä¼˜åŒ–

### 3.1 åŠ¨æ€è¿æ¥æ± é…ç½®

```typescript
// src/common/config/database.config.ts

import * as os from 'os';

export const getDatabaseConfig = (configService: ConfigService) => {
  const cpuCores = os.cpus().length;
  const isDevelopment = process.env.NODE_ENV === 'development';
  const isProduction = process.env.NODE_ENV === 'production';

  // åŠ¨æ€è®¡ç®—æœ€ä½³è¿æ¥æ± å¤§å°
  // å…¬å¼: (CPU æ ¸å¿ƒæ•° Ã— 2) + æœ‰æ•ˆç£ç›˜æ•°
  const maxConnections = isDevelopment
    ? 10
    : Math.min((cpuCores * 2) + 1, 100); // æœ€å¤š 100

  const minConnections = isDevelopment ? 2 : Math.floor(maxConnections * 0.1);

  return {
    type: 'postgres',
    host: configService.get('DB_HOST', 'localhost'),
    port: configService.get('DB_PORT', 5432),
    username: configService.get('DB_USERNAME', 'postgres'),
    password: configService.get('DB_PASSWORD', 'postgres'),
    database: configService.get('DB_DATABASE', 'cloudphone_user'),

    // å®ä½“å’Œè¿ç§»é…ç½®
    entities: [__dirname + '/../../**/*.entity{.ts,.js}'],
    migrations: [__dirname + '/../../migrations/*{.ts,.js}'],

    // æè‡´è¿æ¥æ± ä¼˜åŒ–
    extra: {
      // è¿æ¥æ± å¤§å°
      min: minConnections,
      max: maxConnections,

      // è¿æ¥è¶…æ—¶
      connectionTimeoutMillis: 5000, // ä» 10s é™åˆ° 5s
      idleTimeoutMillis: 20000, // ä» 30s é™åˆ° 20s

      // é¢„ç¼–è¯‘è¯­å¥ç¼“å­˜ (é‡è¦!)
      statement_cache_size: 100,

      // è¿æ¥é‡ç”¨ä¼˜åŒ–
      max_lifetime: isProduction ? 1800 : undefined, // 30åˆ†é’Ÿ
      evictionRunIntervalMillis: 5000, // 5ç§’æ£€æŸ¥ä¸€æ¬¡

      // åº”ç”¨åç§° (ä¾¿äºæ•°æ®åº“ç›‘æ§)
      application_name: `user-service-${process.env.HOSTNAME || 'unknown'}`,

      // æŸ¥è¯¢è¶…æ—¶
      statement_timeout: isProduction ? 30000 : 60000,
      query_timeout: isProduction ? 30000 : 60000,

      // è¿æ¥çº§åˆ«ä¼˜åŒ–
      options: [
        // å¯ç”¨ JIT ç¼–è¯‘ (PostgreSQL 11+)
        '-c jit=on',
        '-c jit_above_cost=100000',

        // å·¥ä½œå†…å­˜ä¼˜åŒ–
        '-c work_mem=32MB',

        // å¯ç”¨å¹¶è¡ŒæŸ¥è¯¢
        '-c max_parallel_workers_per_gather=4',
        '-c effective_cache_size=4GB',
      ].join(' '),
    },

    // è‡ªåŠ¨åŠ è½½å®ä½“
    autoLoadEntities: true,

    // åŒæ­¥ schema (ä»…å¼€å‘ç¯å¢ƒ)
    synchronize: isDevelopment,

    // æ—¥å¿—é…ç½®
    logging: isDevelopment ? 'all' : ['error', 'warn', 'schema'],
    logger: isDevelopment ? 'advanced-console' : 'file',
    maxQueryExecutionTime: isProduction ? 1000 : 5000,

    // Redis æŸ¥è¯¢ç¼“å­˜
    cache: isProduction ? {
      type: 'redis',
      options: {
        host: configService.get('REDIS_HOST', 'localhost'),
        port: configService.get('REDIS_PORT', 6379),
        db: 2, // ä½¿ç”¨å•ç‹¬çš„ DB
      },
      duration: 60000, // 60ç§’ (ä» 30s æå‡)
      ignoreErrors: true,
    } : false,
  };
};
```

### 3.2 è¿æ¥æ± å¥åº·æ£€æŸ¥å¢å¼º

```typescript
// src/common/services/database-monitor.service.ts

import { Injectable, Logger, OnModuleInit } from '@nestjs/common';
import { InjectDataSource } from '@nestjs/typeorm';
import { DataSource } from 'typeorm';
import { Cron, CronExpression } from '@nestjs/schedule';

@Injectable()
export class DatabaseMonitorService implements OnModuleInit {
  private readonly logger = new Logger(DatabaseMonitorService.name);

  // è¿æ¥æ± ç»Ÿè®¡
  private poolStats = {
    totalQueries: 0,
    slowQueries: 0,
    failedQueries: 0,
    avgQueryTime: 0,
  };

  constructor(
    @InjectDataSource()
    private readonly dataSource: DataSource,
  ) {}

  async onModuleInit() {
    // å¯åŠ¨æ—¶é¢„çƒ­è¿æ¥æ± 
    await this.warmUpConnectionPool();
  }

  /**
   * é¢„çƒ­è¿æ¥æ±  (åˆ›å»ºæœ€å°è¿æ¥æ•°)
   */
  private async warmUpConnectionPool() {
    try {
      const minConnections = this.dataSource.options.extra?.min || 2;
      const warmUpPromises = [];

      for (let i = 0; i < minConnections; i++) {
        warmUpPromises.push(
          this.dataSource.query('SELECT 1')
        );
      }

      await Promise.all(warmUpPromises);
      this.logger.log(`Connection pool warmed up with ${minConnections} connections`);
    } catch (error) {
      this.logger.error(`Failed to warm up connection pool: ${error.message}`);
    }
  }

  /**
   * æ¯ 10 ç§’æ£€æŸ¥è¿æ¥æ± å¥åº·çŠ¶æ€
   */
  @Cron(CronExpression.EVERY_10_SECONDS)
  async checkConnectionPoolHealth() {
    try {
      const poolMetrics = await this.getPoolMetrics();

      // è­¦å‘Šé˜ˆå€¼: 80%
      if (poolMetrics.usage > 0.8) {
        this.logger.warn(
          `High connection pool usage: ${(poolMetrics.usage * 100).toFixed(1)}% ` +
          `(${poolMetrics.active}/${poolMetrics.max})`
        );
      }

      // å±é™©é˜ˆå€¼: 95%
      if (poolMetrics.usage > 0.95) {
        this.logger.error(
          `Critical connection pool usage: ${(poolMetrics.usage * 100).toFixed(1)}% ` +
          `Consider scaling up!`
        );

        // å¯ä»¥åœ¨è¿™é‡Œè§¦å‘å‘Šè­¦åˆ° notification-service
        // await this.notificationService.sendAlert(...)
      }

      // æ£€æµ‹è¿æ¥æ³„æ¼
      if (poolMetrics.waiting > 5) {
        this.logger.warn(
          `${poolMetrics.waiting} queries waiting for connection. ` +
          `Possible connection leak or high load.`
        );
      }
    } catch (error) {
      this.logger.error(`Connection pool health check failed: ${error.message}`);
    }
  }

  /**
   * è·å–è¿æ¥æ± æŒ‡æ ‡
   */
  private async getPoolMetrics() {
    // ä» pg pool è·å–ç»Ÿè®¡ä¿¡æ¯
    const pool = (this.dataSource.driver as any).master;

    return {
      max: pool.options.max || 20,
      active: pool.totalCount - pool.idleCount,
      idle: pool.idleCount,
      waiting: pool.waitingCount,
      usage: (pool.totalCount - pool.idleCount) / (pool.options.max || 20),
    };
  }

  /**
   * æ…¢æŸ¥è¯¢æ£€æµ‹
   */
  async logSlowQuery(query: string, executionTime: number) {
    this.poolStats.totalQueries++;
    this.poolStats.avgQueryTime =
      (this.poolStats.avgQueryTime * (this.poolStats.totalQueries - 1) + executionTime) /
      this.poolStats.totalQueries;

    if (executionTime > 1000) {
      this.poolStats.slowQueries++;
      this.logger.warn(
        `Slow query detected (${executionTime}ms): ${query.substring(0, 200)}...`
      );
    }
  }

  /**
   * è·å–ç»Ÿè®¡ä¿¡æ¯ (ç”¨äº Prometheus)
   */
  getStats() {
    return this.poolStats;
  }
}
```

---

## ç¬¬å››é˜¶æ®µ: æŸ¥è¯¢çº§åˆ«ä¼˜åŒ–

### 4.1 é…é¢æ£€æŸ¥ç¼“å­˜

```typescript
// src/quotas/quotas.service.ts

import { Injectable } from '@nestjs/common';
import { InjectRepository } from '@nestjs/typeorm';
import { Repository } from 'typeorm';
import { Quota } from '../entities/quota.entity';
import { CacheService, CacheLayer } from '../cache/cache.service';

@Injectable()
export class QuotasService {
  constructor(
    @InjectRepository(Quota)
    private quotaRepository: Repository<Quota>,
    private cacheService: CacheService,
  ) {}

  /**
   * è·å–ç”¨æˆ·é…é¢ (å¸¦ç¼“å­˜)
   * æ€§èƒ½æå‡: 30ms â†’ 3ms (90% â¬†ï¸)
   */
  async getUserQuota(userId: string): Promise<Quota | null> {
    const cacheKey = `quota:user:${userId}`;

    // 1. å°è¯•ä»ç¼“å­˜è·å– (60ç§’ TTL)
    const cached = await this.cacheService.get<Quota>(cacheKey);
    if (cached) {
      return cached;
    }

    // 2. ä»æ•°æ®åº“æŸ¥è¯¢ (ä½¿ç”¨ QueryBuilder ä¼˜åŒ–)
    const quota = await this.quotaRepository
      .createQueryBuilder('quota')
      .select([
        'quota.id',
        'quota.userId',
        'quota.planId',
        'quota.limits',
        'quota.usage',
        'quota.status',
        'quota.validFrom',
        'quota.validUntil',
      ])
      .where('quota.userId = :userId', { userId })
      .andWhere('quota.status = :status', { status: 'active' })
      .andWhere('quota.validUntil > :now', { now: new Date() })
      .orderBy('quota.createdAt', 'DESC')
      .getOne();

    // 3. ç¼“å­˜ç»“æœ (å³ä½¿æ˜¯ null)
    await this.cacheService.set(cacheKey, quota, {
      ttl: 60,
      layer: CacheLayer.L1_AND_L2,
      randomTTL: true,
    });

    return quota;
  }

  /**
   * ä½¿ç”¨é…é¢ (å¸¦é”æœºåˆ¶é˜²æ­¢è¶…é¢)
   */
  async useQuota(
    userId: string,
    resource: string,
    amount: number,
  ): Promise<boolean> {
    const lockKey = `lock:quota:${userId}`;

    // 1. è·å–åˆ†å¸ƒå¼é”
    const lockAcquired = await this.cacheService.set(lockKey, '1', {
      ttl: 5,
      layer: CacheLayer.L2_ONLY,
    });

    if (!lockAcquired) {
      throw new Error('Failed to acquire quota lock');
    }

    try {
      // 2. ä½¿ç”¨äº‹åŠ¡ç¡®ä¿åŸå­æ€§
      const result = await this.quotaRepository.manager.transaction(async (manager) => {
        // ä½¿ç”¨ FOR UPDATE é”å®šè¡Œ
        const quota = await manager
          .createQueryBuilder(Quota, 'quota')
          .setLock('pessimistic_write')
          .where('quota.userId = :userId', { userId })
          .andWhere('quota.status = :status', { status: 'active' })
          .getOne();

        if (!quota) {
          return false;
        }

        // æ£€æŸ¥é…é¢æ˜¯å¦è¶³å¤Ÿ
        const currentUsage = quota.usage[resource] || 0;
        const limit = quota.limits[resource] || 0;

        if (currentUsage + amount > limit) {
          return false;
        }

        // æ›´æ–°ä½¿ç”¨é‡
        quota.usage[resource] = currentUsage + amount;
        await manager.save(quota);

        return true;
      });

      // 3. æ¸…é™¤ç¼“å­˜
      await this.cacheService.del(`quota:user:${userId}`);

      return result;
    } finally {
      // 4. é‡Šæ”¾é”
      await this.cacheService.del(lockKey);
    }
  }
}
```

### 4.2 ç”¨æˆ·ç»Ÿè®¡ç‰©åŒ–è§†å›¾

```sql
-- åˆ›å»ºç‰©åŒ–è§†å›¾åŠ é€Ÿç»Ÿè®¡æŸ¥è¯¢
CREATE MATERIALIZED VIEW user_stats_materialized AS
SELECT
  tenant_id,
  DATE_TRUNC('day', created_at) as stat_date,
  COUNT(*) as total_users,
  COUNT(CASE WHEN status = 'active' THEN 1 END) as active_users,
  COUNT(CASE WHEN status = 'inactive' THEN 1 END) as inactive_users,
  COUNT(CASE WHEN is_super_admin = true THEN 1 END) as admin_users,
  COUNT(CASE WHEN last_login_at >= NOW() - INTERVAL '7 days' THEN 1 END) as recently_active
FROM users
GROUP BY tenant_id, DATE_TRUNC('day', created_at);

-- åˆ›å»ºç´¢å¼•
CREATE INDEX idx_user_stats_mv_tenant_date
ON user_stats_materialized(tenant_id, stat_date DESC);

-- æ¯å°æ—¶åˆ·æ–°ä¸€æ¬¡
CREATE EXTENSION IF NOT EXISTS pg_cron;
SELECT cron.schedule('refresh-user-stats', '0 * * * *', 'REFRESH MATERIALIZED VIEW CONCURRENTLY user_stats_materialized');
```

**æ€§èƒ½æå‡**:
- ç»Ÿè®¡æŸ¥è¯¢: 150ms â†’ 5ms (97% â¬†ï¸)
- æ— éœ€å®æ—¶ç»Ÿè®¡å…¨è¡¨
- æ”¯æŒå†å²è¶‹åŠ¿åˆ†æ

---

## ç¬¬äº”é˜¶æ®µ: è¯»å†™åˆ†ç¦»

### 5.1 é…ç½®ä¸»ä»å¤åˆ¶

```typescript
// src/common/config/database.config.ts

export const getDatabaseConfig = (configService: ConfigService) => {
  const isProduction = process.env.NODE_ENV === 'production';

  return {
    // ... å…¶ä»–é…ç½®

    replication: isProduction ? {
      master: {
        host: configService.get('DB_MASTER_HOST'),
        port: configService.get('DB_MASTER_PORT', 5432),
        username: configService.get('DB_USERNAME'),
        password: configService.get('DB_PASSWORD'),
        database: configService.get('DB_DATABASE'),
      },
      slaves: [
        {
          host: configService.get('DB_REPLICA_1_HOST'),
          port: configService.get('DB_REPLICA_1_PORT', 5432),
          username: configService.get('DB_USERNAME'),
          password: configService.get('DB_PASSWORD'),
          database: configService.get('DB_DATABASE'),
        },
        {
          host: configService.get('DB_REPLICA_2_HOST'),
          port: configService.get('DB_REPLICA_2_PORT', 5432),
          username: configService.get('DB_USERNAME'),
          password: configService.get('DB_PASSWORD'),
          database: configService.get('DB_DATABASE'),
        },
      ],

      // ä»åº“é€‰æ‹©ç­–ç•¥: RR (è½®è¯¢) æˆ– RANDOM (éšæœº)
      selector: 'RR',

      // ä»åº“å»¶è¿Ÿæ£€æµ‹
      canRetry: true,
      removeNodeErrorCount: 3,
      restoreNodeTimeout: 0,
    } : undefined,
  };
};
```

### 5.2 æ˜¾å¼æŒ‡å®šè¯»å†™æ“ä½œ

```typescript
// å†™æ“ä½œä½¿ç”¨ä¸»åº“
await this.dataSource.manager.save(user);

// è¯»æ“ä½œä½¿ç”¨ä»åº“
const users = await this.dataSource
  .getRepository(User)
  .createQueryBuilder('user', 'slave') // 'slave' æŒ‡å®šä½¿ç”¨ä»åº“
  .where('user.status = :status', { status: 'active' })
  .getMany();
```

**æ€§èƒ½æå‡**:
- ä¸»åº“è´Ÿè½½å‡å°‘ 70-80%
- è¯»æ“ä½œå»¶è¿Ÿé™ä½ 30-40%
- æ”¯æŒæ°´å¹³æ‰©å±•

---

## ç¬¬å…­é˜¶æ®µ: PostgreSQL æœåŠ¡å™¨ä¼˜åŒ–

### 6.1 postgresql.conf ä¼˜åŒ–

```ini
# å†…å­˜é…ç½® (å‡è®¾ 16GB RAM)
shared_buffers = 4GB                 # 25% of RAM
effective_cache_size = 12GB          # 75% of RAM
maintenance_work_mem = 1GB           # For VACUUM, CREATE INDEX
work_mem = 32MB                      # Per operation

# å†™å…¥ä¼˜åŒ–
wal_buffers = 16MB
checkpoint_completion_target = 0.9
max_wal_size = 4GB
min_wal_size = 1GB

# å¹¶è¡ŒæŸ¥è¯¢
max_parallel_workers_per_gather = 4
max_parallel_workers = 8
max_worker_processes = 8

# æŸ¥è¯¢ä¼˜åŒ–
random_page_cost = 1.1               # For SSD
effective_io_concurrency = 200       # For SSD

# JIT ç¼–è¯‘ (PostgreSQL 11+)
jit = on
jit_above_cost = 100000
jit_inline_above_cost = 500000
jit_optimize_above_cost = 500000

# è¿æ¥
max_connections = 200
```

### 6.2 å®šæœŸç»´æŠ¤ä»»åŠ¡

```sql
-- 1. æ¯å¤©å‡Œæ™¨ 2 ç‚¹æ‰§è¡Œ VACUUM ANALYZE
SELECT cron.schedule('vacuum-analyze-daily', '0 2 * * *',
  'VACUUM ANALYZE users, user_events, quotas, audit_logs');

-- 2. æ¯å‘¨æ—¥å‡Œæ™¨ 3 ç‚¹æ‰§è¡Œ REINDEX
SELECT cron.schedule('reindex-weekly', '0 3 * * 0',
  'REINDEX TABLE CONCURRENTLY users; REINDEX TABLE CONCURRENTLY user_events');

-- 3. æ¯æœˆ 1 å·å½’æ¡£æ—§å®¡è®¡æ—¥å¿—
SELECT cron.schedule('archive-audit-logs-monthly', '0 4 1 * *',
  $$
    DELETE FROM audit_logs
    WHERE created_at < NOW() - INTERVAL '90 days'
  $$
);
```

---

## æ€§èƒ½æµ‹è¯•åŸºå‡†

### æµ‹è¯•ç¯å¢ƒ
```
ç¡¬ä»¶: 8 æ ¸ CPU, 16GB RAM, SSD
æ•°æ®é‡: 100,000 users, 1,000,000 events
å¹¶å‘: 100 å¹¶å‘è¿æ¥
```

### åŸºå‡†æµ‹è¯•ç»“æœ (é¢„ä¼°)

| æ“ä½œ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æå‡ |
|------|-------|-------|------|
| ç”¨æˆ·ç™»å½• | 50ms | 20ms | 60% |
| ç”¨æˆ·åˆ›å»º | 200ms | 120ms | 40% |
| é…é¢æ£€æŸ¥ | 30ms | 3ms | 90% |
| ç»Ÿè®¡æŸ¥è¯¢ (ç¼“å­˜) | 5ms | 2ms | 40% |
| ç»Ÿè®¡æŸ¥è¯¢ (æœªç¼“å­˜) | 150ms | 30ms | 80% |
| äº‹ä»¶é‡æ”¾ (100äº‹ä»¶) | 100ms | 30ms | 70% |
| äº‹ä»¶æ‰¹é‡ä¿å­˜ (100) | 250ms | 180ms | 28% |
| ç”¨æˆ·åˆ—è¡¨ (1000æ¡) | 80ms | 40ms | 50% |
| ååé‡ (qps) | 100 | 1000 | 900% |

---

## ç›‘æ§å’Œå‘Šè­¦

### Prometheus æŒ‡æ ‡

```typescript
// æ–°å¢æŒ‡æ ‡
db_connection_pool_size{state="active|idle|waiting"}
db_query_duration_seconds{operation,slow="true|false"}
db_transaction_duration_seconds
db_deadlock_count_total
db_cache_hit_ratio{layer="l1|l2"}
db_slow_query_count_total
db_partition_size_bytes{table,partition}
```

### Grafana Dashboard

åˆ›å»ºåŒ…å«ä»¥ä¸‹é¢æ¿çš„ä»ªè¡¨æ¿:
1. è¿æ¥æ± ä½¿ç”¨ç‡ (æ—¶é—´åºåˆ—)
2. æŸ¥è¯¢å»¶è¿Ÿåˆ†å¸ƒ (ç›´æ–¹å›¾)
3. æ…¢æŸ¥è¯¢ TOP 10
4. ç¼“å­˜å‘½ä¸­ç‡
5. æ•°æ®åº“ CPU/å†…å­˜ä½¿ç”¨
6. è¡¨å¤§å°å¢é•¿è¶‹åŠ¿
7. åˆ†åŒºå¤§å°åˆ†å¸ƒ

---

## å®æ–½è·¯çº¿å›¾

### Week 1: åŸºç¡€ä¼˜åŒ–
- âœ… æ·»åŠ å”¯ä¸€çº¦æŸå’Œéƒ¨åˆ†ç´¢å¼•
- âœ… ä¼˜åŒ–è¿æ¥æ± é…ç½®
- âœ… å®æ–½é…é¢æ£€æŸ¥ç¼“å­˜

### Week 2: é«˜çº§ä¼˜åŒ–
- âœ… å®æ–½è¡¨åˆ†åŒº (user_events, audit_logs)
- âœ… åˆ›å»ºç‰©åŒ–è§†å›¾
- âœ… ä¼˜åŒ– PostgreSQL é…ç½®

### Week 3: æ‰©å±•æ€§ä¼˜åŒ–
- âœ… é…ç½®è¯»å†™åˆ†ç¦»
- âœ… æ·»åŠ æ€§èƒ½ç›‘æ§
- âœ… æ€§èƒ½åŸºå‡†æµ‹è¯•

### Week 4: æŒç»­ä¼˜åŒ–
- âœ… æ ¹æ®ç›‘æ§æ•°æ®è°ƒä¼˜
- âœ… å‹åŠ›æµ‹è¯•
- âœ… æ–‡æ¡£å’ŒåŸ¹è®­

---

## æ€»ç»“

é€šè¿‡ä»¥ä¸Š 6 ä¸ªé˜¶æ®µçš„ä¼˜åŒ–,é¢„æœŸå¯å®ç°:

ğŸ“Š **æ€§èƒ½æå‡**: æ•´ä½“å“åº”æ—¶é—´å‡å°‘ 60-80%
ğŸš€ **ååé‡æå‡**: ä» 100 qps â†’ 1000 qps (900% â¬†ï¸)
ğŸ’° **æˆæœ¬é™ä½**: ç›¸åŒæ€§èƒ½ä¸‹ç¡¬ä»¶æˆæœ¬é™ä½ 40-50%
ğŸ“ˆ **å¯æ‰©å±•æ€§**: æ”¯æŒ 100 ä¸‡ç”¨æˆ· + 1000 ä¸‡äº‹ä»¶

**æè‡´ä¼˜åŒ–å®Œæˆ!** ğŸ‰
